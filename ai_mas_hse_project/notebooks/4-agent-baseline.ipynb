{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "856b6081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tas/.cache/pypoetry/virtualenvs/ai-mas-hse-project-Nc-XwFR1-py3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint, HuggingFacePipeline\n",
    "from langchain.tools import tool\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated\n",
    "\n",
    "load_dotenv()  # Считываем креды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74d193d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (NVIDIA GPU) is available.\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA (NVIDIA GPU) is available.\")\n",
    "elif torch.backends.mps.is_available():  # For Apple Silicon (M1/M2/M3)\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS (Apple Silicon GPU) is available.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU found, running on CPU.\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "595a8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данная команда позволяет очистить неиспользуемый кэш в GPU\n",
    "# В моем случае, если я несколько раз инициализирую модель через HuggingFacePipeline, то у меня несколько раз задублируется память на GPU\n",
    "# Если запустить команду torch.cuda.empty_cache() , то на GPU останется только один инстанс модели, а не несколько\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf661851",
   "metadata": {},
   "source": [
    "# Загружаем датасет с математическими задачами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5f9e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/final_dataset/list_dict_with_tasks.pkl\", \"rb\") as file:\n",
    "    list_json = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576bb49d",
   "metadata": {},
   "source": [
    "## Фиксируем одну конкретную задачу для тестирования бейзлайна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afc5d388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 32838,\n",
       " 'topic': 'Алгебра и арифметика',\n",
       " 'subtopic': 'Задачи на движение',\n",
       " 'complexity_level': '2',\n",
       " 'problem': 'Вадим и Лёша спускались с горы. Вадим шёл пешком, а Лёша съезжал на лыжах в семь раз быстрее Вадима. На полпути Лёша упал, сломал лыжи и ногу и пошёл в два раза медленней Вадима. Кто первым спустится с горы?',\n",
       " 'solution': 'Леша шёл пешком с половины горы столько же времени, сколько Вадим спускался с вершины.',\n",
       " 'answer': 'Вадим'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_task = list_json[47]\n",
    "test_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b95be2",
   "metadata": {},
   "source": [
    "# Hugging Face. Инициализируем модель\n",
    "* [Huggingface Endpoints](https://docs.langchain.com/oss/python/integrations/llms/huggingface_endpoint#dedicated-endpoint) - здесь описано как пользоваться `HF Endpoints`. Также приведены ссылки с информацией об `inference-providers`. В параграфе `Dedicated Endpoint` описано в каких случаях инференс через endpoint бесплатный, а в каких платный.\n",
    "* [Inference Providers](https://huggingface.co/docs/inference-providers/index) - исчерпывающая информация о провайдерах.\n",
    "* [Inference Cost](https://huggingface.co/settings/inference-providers/overview) - здесь показывается информация о том сколько запросов я сделал и с помощью какой LLM. Также показывается сколько денег я должен заплатить за произведенные запросы. Также тут можно посмотреть (переключив ползунок в положение Providers) услугами каких провайдеров я пользуюсь при выполнении запросов к LLM.\n",
    "* [Billing](https://huggingface.co/settings/billing) - здесь можно посмотреть более подробную сводку о денежных счетах и их оплате. Согласно этой странице, дефолтный HuggingFaceEndpoint, который я использую ниже - бесплатный. Потому что в графе Inference Usage написано 0.00$.\n",
    "    - Также тут написаны ограничения в графе Hub Rate Limits. Каждый раз, когда я обновляю страницу лимиты сокращаются. Непонимаю почему так, потому что API не пользуюсь в данный момент.\n",
    "    - А, нет. Все нормально. Я ушел с этой страницы, а потом опять на нее зашел и все стало нормально. Нельзя превышать определенное число запросов за последние 5 минут. В остальном все бесплатно.\n",
    "* [Inference Endpoints](https://endpoints.huggingface.co/) - сервис для платного инференса (соответственно с большими лимитами)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de24ab69",
   "metadata": {},
   "source": [
    "## HuggingFaceEndpoint\n",
    "Обращаемся к модели через бесплатный API. Этот метод имеет ряд ограничений по вызовам к модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ccad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=\"deepseek-ai/DeepSeek-R1-0528\",\n",
    "#     task=\"text-generation\",\n",
    "#     max_new_tokens=512,\n",
    "#     do_sample=False, # При do_sample=True становятся доступны параметры temperature, top_k, top_p\n",
    "#     repetition_penalty=1.03,\n",
    "#     provider=\"auto\",  # let Hugging Face choose the best provider for you\n",
    "# )\n",
    "\n",
    "# hf_deepseek_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c660e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "I need to calculate 3 raised to the power of 3. That's 3^3. I know that exponentiation means multiplying the base number by itself as many times as the exponent says. So for 3^3, I should multiply 3 by itself three times.\n",
      "\n",
      "Let me write that out: 3 × 3 × 3.\n",
      "\n",
      "First, 3 times 3 is 9. Then, 9 times 3 is 27. So, 3^3 should be 27.\n",
      "\n",
      "I recall that 3^2 is 9, and 3^3 is 3 times 3^2, so 3 times 9 is 27. That matches.\n",
      "\n",
      "Also, 3^1 is 3, 3^2 is 9, 3^3 is 27. It seems consistent.\n",
      "\n",
      "I think I've got it. So the answer should be 27.\n",
      "\n",
      "The problem says \"calculate 3^3\", and I've done that. I don't think there's any trick here. It's straightforward exponentiation.\n",
      "\n",
      "Just to be thorough, let me consider what exponentiation means. 3^3 = 3 × 3 × 3, which is indeed 27.\n",
      "\n",
      "In terms of repeated multiplication, it's clear.\n",
      "\n",
      "I think I'm good.\n",
      "</think>\n",
      "\n",
      "To calculate \\(3^3\\), which means 3 raised to the power of 3, multiply 3 by itself three times:\n",
      "\n",
      "\\[\n",
      "3 \\times 3 \\times 3\n",
      "\\]\n",
      "\n",
      "First, \\(3 \\times 3 = 9\\).\n",
      "\n",
      "Then, \\(9 \\times 3 = 27\\).\n",
      "\n",
      "Therefore, \\(3^3 = 27\\).\n",
      "\n",
      "Alternatively, \\(3^3\\) can be computed as \\(3 \\times 3^2\\), and since \\(3^2 = 9\\), it follows that \\(3 \\times 9 = 27\\).\n",
      "\n",
      "The result is 27.\n"
     ]
    }
   ],
   "source": [
    "# response = hf_deepseek_model.invoke(\"Calculate 3^3\")\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c12d83",
   "metadata": {},
   "source": [
    "## HuggingFacePipeline\n",
    "Данный метод скачивает модель на локальный компьютер по следующему пути: `/home/tas/.cache/huggingface/hub`\n",
    "* Из плюсов: нет никаких ограничений. Все ограничения упираются в лимиты по ресурсам локального компьютера.\n",
    "* Я выбрал самую легковесную модель: `TinyLlama/TinyLlama-1.1B-Chat-v1.0`. На GPU она занимает 4гб из 12гб."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b04cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task=\"text-generation\",\n",
    "    device=0,\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,  # При do_sample=True становятся доступны параметры temperature, top_k, top_p\n",
    "        repetition_penalty=1.03,\n",
    "    ),\n",
    ")\n",
    "\n",
    "tiny_lama_model = ChatHuggingFace(llm=llm)\n",
    "# tiny_lama_model_with_tools = tiny_lama_model.bind_tools([multiply]) # Привязываем Tools к LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "640d217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Calculate 3^3</s>\n",
      "<|assistant|>\n",
      "To calculate 3^3, we need to follow these steps:\n",
      "\n",
      "1. Multiply the base (3) by the exponent (3).\n",
      "2. Divide the result by 3 to get the quotient.\n",
      "3. Multiply the quotient by 3.\n",
      "4. Add the result to the original base.\n",
      "\n",
      "So, to calculate 3^3, we would do the following:\n",
      "\n",
      "1. Multiply 3 by 3.\n",
      "2. Divide the result by 3.\n",
      "3. Add the result to 3.\n",
      "\n",
      "The result is 9, which is the value of 3^3.\n"
     ]
    }
   ],
   "source": [
    "response = tiny_lama_model.invoke(\"Calculate 3^3\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d6f554",
   "metadata": {},
   "source": [
    "# Строим task_solver_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8096c3f",
   "metadata": {},
   "source": [
    "Модельки для тестирования запросов:\n",
    "* `TinyLlama/TinyLlama-1.1B-Chat-v1.0` - мелкая, быстрая, но очень глупая\n",
    "* `LiquidAI/LFM2-2.6B-Exp` - потяжелее, поумнее, помедленней и плоховато с русским языком\n",
    "* `HuggingFaceTB/SmolLM3-3B` - не влезает\n",
    "* `utter-project/EuroLLM-1.7B-Instruct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aba9c23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]\n",
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"LiquidAI/LFM2-2.6B-Exp\",\n",
    "    task=\"text-generation\",\n",
    "    device=0,\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=8192,\n",
    "        do_sample=False,  # При do_sample=True становятся доступны параметры temperature, top_k, top_p\n",
    "        repetition_penalty=1.03,\n",
    "    ),\n",
    ")\n",
    "\n",
    "tiny_lama_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67a5ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e87dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node\n",
    "def task_solver_llm(state: MessagesState):\n",
    "    return {\"messages\": [tiny_lama_model.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1097ad03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJwAAADqCAIAAADYobvGAAAQAElEQVR4nOydCXwMZx/Hn9kzm93c90kutyaIsypphCqlVCmi9Za+6qyzRWnrqKIo7+uoeut6tWhFlVbreFEkiijiiCsigiTItclmN3vMzPvfXdYmmQ1bZnYzM19tPrPP88wzs/Ob/3M//xWRJIl42IUI8bAOXlQWwovKQnhRWQgvKgvhRWUhTiTqjcyK7PMVxQU6XIegmwVdLaEIww0kJkAYwh51vTAMQwRhPIZwAYbBMcRgAgxCSIKEWDgiHnbTMEgDgZCMhAwIYwikEgiMZ0FGcKYpgfF00nQORJGmfB6eLzBej8AhmYAkCGMsWa0PKHHBBEJM7i4KbSKL6+yFnAPM4f3Uvw4WX0gvr1TicCMSF2OIWCIUiTESxwQijDCJarxPRBr/R48EMApp1MgolfkYmeSBEOM/k8ZGkUwJMJOEhOn0RzmYNTWmN51uktwYBS8J9uiRPM7WrL3w4aUtSGRIW0UadIReS+I46eIqCG8q6zYkCDkUR4p65mDx6YNluJ70C5G0TvKKfMEN1WdK7mv//LX4brYa16PwZq693g1GDsJhom6cc1Otwpu1UyQOCETs4kqGMn1XMU6QKTPD5HIJYhzHiLpmWraXv+StKeGIvRxOvZf1Z0V8smf7V30RszhA1JWTs1/q5xX7kg/iACunZL81KcQvVIYYhGlRV03JTpke7OnnijjDmo+yYxM9O/Zkzl4FiEFWT81OGODNKUWBUV9Gnz1UdidbhZiCOVE3zbvpFypp0cEbcY/2r3nuWVuImIIhUf86VKKuwAdMZHPLqA7aJPrKPYXb/3UbMQJDombsK2kcr0AcZsDE0Pu3tIgRmBD13NESgkBJA9nWH7ULqatY4SlMZcRYGRH1sNI3RIw4T8vOHvdvM2GsTIiqUuKtEpke7O7Wrdvdu3eRndy4ceO1115D9NA6yRvGjbMzKxDN0C7qjfNKmEGJiXNHDFJQUFBaWorsJysrC9GJi1yQdUKJaIb2qbecCxqRFEP0ACMnW7du/fXXX2/duhUREdGhQ4fRo0efPXt21KhREPv6668nJCQsXboU7C81NTUjIyM/Pz8yMrJv375vvvmmOYeuXbu+9957hw4dgrPefvvtzZs3Q2B8fPykSZNSUlLQ88bNS6R8oEc0Q7uoyiKdVCZE9LBt27b169dPnDjxxRdf/OOPP1atWiWXy999993ly5dD4K5du0JCQiAZ6Apyzpw5E2bacnNzFy1aFBQUBKdAlFgs3rlzZ7t27UDaNm3aQIL9+/fDW4Lowc1XpCyp/6LCRKNEQpelnjlzplmzZuZasF+/fm3btlWr1bWTLViwoLKyMjjYOBcGVrh79+7jx4+bRQUVPTw8pk6dihhB4S4mdBpEM7SLClPOiC5NUWxs7IoVK+bOnduqVasuXbqEhoYi6nsgwabT09OhlDaHmC3YDLwWiCkwJCBpexoWaBdVLEUaFYHoYciQIVDeHjlyZM6cOSKRCFq8H3zwgZ+fn3UagiAmTJig0+nGjRsHZurm5jZixAjrBBIJc1OeqgothtE+g0K7qApoGhRVIXoQCAT9TOTk5Jw6dWrt2rUqlWrZsmXWaa5cuXLp0qXVq1dDxWkOqaio8Pf3R45A+cAgktDe46D9AmFNXHVVdFkqtGigZQsH0KYdNGjQ4MGDr169WiNNWVkZ/LWomGMCOQhlsc7Nm3ZDol3UFu09CRzdu01L62Dv3r0ffvjh0aNHlUplWloa9EygloXwhg0bwt8DBw5cvHgR9IaSGfoq5eXl0PRdvHgx9HygI0uZYXh4eFFRETSkLbXv80VbiWJayxHNMDGiJJVh6buKEA3MmjULNJs8eTJ0N+fNmwe9Uui3QDi0mHr37r1mzRpoRgUGBn7++ecXLlxISkqC3ufYsWOhkwpiW7qq1nTu3DkuLg4aw/v27UPPm2tnjWNJcfQv+WBi5cORHQ8upivHfhWNuM13X+RCF2rojAaIZpiw1IT+xuZoxoFixG3KHhh6jmBiqoqhFfpN2spPHyht24265Ll///7AgQMpoxQKBTRoKaOg4IXhJEQPG01QRpmX9lNGjRkzxtYX+X5RrtwT8/aXIvphbuHZ2hk54U1kPYZRrF6HriSM+FCeBf1LW/1IeLggOaIHrVYLl6aM0mg0Mhn16kCpVEp5t4W3K1O/Khi3jKEKiLm9NCMXRK6anK1WGVwVNS8K3U0YE0DOhNQEZdTfuNWdKwraJHsipmB0NWH3t/03zc5FHOPbT7KDI1069mJuiSjT634f5Gt+WHyXsYLI4ayZlt3uFW/j9DiDOGCFfs5F1W/rClt0dkvsH4DYy+3Lqj0bCgMiXPqNDkXM4pi9NHDR/3ycIxJj3VICwhrTPsLCPFu/vFV6X9+uh2d8MtMbaZBjtzL+8p+7t69qpK6CmDhFlzccM8L+fMlMK71wtKysGPfyE6dMp32QwRaO33T8y9o7+Tlag54USTAXV6Gru8BFJoSpDNJqGvbh1mDj7mLMeGD6aMG4URw9/B6WKON2YtP2YOtw679W2WICIYnj5qyMu9AtF7K+AXNaU46PcxAgoqqK0KhwtRKv0hBCIeblL+o7KlAqZ6I/agvHi2qmpFBz5pCy8JZWozLgekQY943XFNXS76/R/ceQ+ZFbpTDtGDeJY1bWmAOOw0MXms5/GPjoAERFBP74dGNw9RcHwx7tH6/+6giEmEiMJDLMJ1Aa3UbeKNYDOQHOIioDvPrqq5s2bXLUTCqTcMg7i8FggDk4xAF4UVkILyoL4ZCoer1eLObElh7eUlkIV0TFcRzmgkydV/bDFVG5Y6aIO6Jyp0JFvKWyEl5UFsKLykJ4UVkILyoL4UVlIbyoLIQXlYXwgw8shLdUFsKLykJ4UVkILyoL4RtKLIS3VBbCle8pEAjc3Rn1ZOpAuCIqhmF/z1lsfYQzJZJIBCUw4ga8qCyEF5WF8KKyEA6Jipu3oHIArogqFAp5S2UbfPHLQnhRWQgvKgvhRWUhvKgshBeVhfCishBeVBbCi8pCOCUqyz2ezZ8/PzU1FTOB0ENXdzBkmJGRgdgLo565mWfkyJERERFmFx6AwER4eLgtl/3sgOWi+vn5de/eHYS0hIC0vXr1kstZ6GTYAstFBYYOHdqgwWPXu6GhoX379kWshv2iKhSKPn36WOrUpKQkb29GPdozD/tFBQYPHmz+vVz4279/f8R2ntz6zbtWef1MhbbWT6DWcI9tBlokJFkzS+pAk4vtWoHGvwRZ80Jmf9ok+eQbMPpzJhGqFXX7zu1rV68GBQU1a9a81jkkVivzR9elyMoSRfngLD7VKJ8r5bd+ylhALCF9wyStnvQTgE8Qdd2n2Vo1EksFem1tAZHJy3n1MCEiCapAEiOrayUw+sgW1AjEjJ7TSQpRTX62ayY2PT6S8g2Ax1P9N1sxASIIUlAr3PiD0ibtKJQTPJS15g2Zcnvo4JtSb7PklKJSfevHsVRvvzViF8ygJ+DV7z0yODjS1VayukT9Znq2b4io+zsNEY8zcT6tKPNwWb9xwUENqXW1Kep/ZmaHxrh07sf0b6rwPA06nW7boryxS6IpY6kbSn/+ep/AEa+o0yKRSBRegm1LciljqUXNu17l4sYhV8D1kYBwuaqUetErtXJ6NYHo+nF4nueDi1yk01FXndSi4tCCJTjh77j+QoKV2jA8voytt9joNSFbdSpnHJPXZzDSlkjUotoYLeFxJkib3VHq4rf2qBCPswFWKhBQWypfp9ZXCOMguY3hRspQvkZ1fozzFnY1lLjzU431GdKW6VEXvzBBwVeqTo/NSTpbDSXzT8PyODECk+1Rx1CGCjC+WnV2SITs6tKgRz/9zOO82K4fqS2VIGxNztPFZ7M/mjJ1NKKZ2XOmTf1wDKKZnJzsl7vGX7hwDo53/LQtuXt7RAuYfcOEmADZW/ru/PnHBYs+QzyMgdnZ+v0bI0pXr2YhHiYh7Wz92svEySMzM8/Awf79e75Z812jmCY/7fzhxIljly9flEilsS+0HjFibEiwcR1Fhapiw8Y1J0+klZaVNG7ULDn51V49ay6tLi4uGjXm7WZNW87+bFEd7bU6skpPP7Lpv2tv5d308PCMjm48Yfy0gIBAy4mVlZV93+g67J2RQ1OGm0NwHO/T9+XX+wwY+c/xJSXFq7/+6uKlzKqqqrZtO74z9L2wsAbIVJBu2bph0sQZUFP07Ttw/NipyE76vpH8j2Hv37mTt+OnrZ6eXh07vDRu7NQvFn4CdwuXGDpkePfuvezIzt7Wr0iMCYR2lL/Lv1rbtGkLuKfDB0+DolCdrFi5uHnz2Llzl0yfNqe0tGT+F7PMKb/8ck7WpfMTJ87YuD4VTlm2fMGlS+ets9JoNB9NH+fj7Tvz48/rboHbyur0Xyc/nf0h3MyP23777JOF9+4VLP/3QusT5XI5PNBjxw5ZQuAUtVrdNakHqDtpyvvnMv+aNPHj9d/+4OXpPWbssLv5d5BpBYlaXbl7d+qM6XP7vT4Q2Y9YLN72w6bw8Ib7fj/+3oixv+/dPWnySLjogX0nXk7stnjpPHhN7cjOduuXWlSDniTwv99Satas5YZ1P6YMebdVXHzb+A4DBwwFk1WWKyEq8/yZLl26QqC/fwCYxaqVG318/CwnwjP95NMp6srKhQv+DQ+x7qvYymr9hq+7vJT0Zv8hYKbNm78wZvTkEyfSrlSvHRISkq9dv1JQmG/+mJZ2uGHDyKioGHgd8/JyP54xr327Tt7ePqNHTXT38NyxYwsyDZ2C7Q4aNCy5a4/Q0HD0t4iJbtKnd3/4aokJ3eAj3B7IKRKJXk7sbjAY8m7dtCMv2/OptovfZxhREgqF+fl3Vq1eevnKRcv+srLSEg93j5Yt437c/p1SWQZlMhRujRs1Ncead6V9uWTulauXvl71XyidnngVW1nl5FxP6NLVkgxKZvh75cqlJo2bWQJf7JQglUrBWOGFg3GWI0cPwgGEX7h4Duypdau2lruKi20Db4/lxCaNm6NnAMzUfGDeodWwYZT5o0xmXOxZUVGO7MDOhhIMPjxLlwYqiVmfTgFLfX/kBHj9oXD7aNo4c9S0j2ZDCXbo8D7QQyFX9Ov31jtv/xNeVXiy8OzgbXVTuEmlLk9zFcqswJi0Wq11Dq6uxucFJaf1uS4uLp06djmWdhi0BOuEp9ktuSeEq1QVer0eOiTWia3fsCeWH3VTo0Kx3o5nPzbPpRaVeLZhwl9/2wlmBNWG+aPKqqpwd3OH5gnoffFiJjzTzd+tUyjczFYilytmf7po6bL5Cxd9tnTJ108c0qLM6o1+gyCqqkpjSVZpkhMq6RqnJyZ2gyYPNMqOHjsExaC5JeXj4yuTyeZ/vsw6pVAgRE6IgER2rXwQigR2NZRqUF6u9PP1t3y0NEmgWoVWMRgTCAaqjxk9CSpdqNvMsVGRMXFxbeZ89iWUgd9v2VD3JWxlBUYP5bB148t8HBkVUyMHaCtBGXjiTiA7TgAACthJREFUZBrYOrRWHt5DVCNoqfn7B0Ju5v8CAoKg/YycEHtHlHADYW9DKSQkDFpDZ85mQFs3OqpRxukTZ8+dhuJ0e+r35gSF9wpEQhH0NGbPnQa2BT0H6P9cz77SskWcdT6RkdH/fG/cxk3fWMSmpI6s+vV9Ky39jx07tpZXlMM9QP8E6siYWsJA3dmpUwIU4FArJyYkmwPbtG7Xrl2nJUvm3btXCOE/79o+avTbe/fuRk5Ire1JFp7byofevd64du3yhx+NXbRwxfDhY6AOm/XJZHjroTyEXk1Bwd3pMz6AXsrc2YtXrFo8fsIIOCUiImrU+xNf7dGnRlZQGp86dXz27I/WffsDFIaUlwMjs5UVdGYeFN3/YfvmlauXQqEa36YDvCWUmSR2SZ55YDK0n728Hu9YXTB/+e5fdsz9fEZW1gXoPkL39403BqF6BfXipU3zckkc9Z/UEPE4K6f3F10+oRyzNKp2lG1L5afenBwQyL4uDYZwJ9C0d59EW1HTps3u/GIichDQC/p45kRbsd9t/hnGPRDdEMi+OtW4nN8JplPXrt1iKwoG8JDjgPZ2HffGhKKorsXc1KIKMUzvBJYaFBiMnBXH35vtxdw2ujQEifELH5wcm1WqrdWEfCPJ+YGGj70r9HlDdXZw0r6GEsYvJqzP1DGixKtaX6ljlgbxODUCZGvSxWbxi3icHALZmnSxUfxifOlbj7Gx640vfusz1JYqkQlJA1d+mLK+IiBEUntWPsjkqKqKF9WpKb1XJRLbM0z48kBfjYovf52a4nxdg6bUTuOpRfXwkQVGSL5fkI14nJKdK3OEIix5cBBlbF2uYU/ue3DmoDIo0jUkRiZzfYqlkRjF6KJ1GGlZ/4Y9h3FISx6keQOYVTfMKnvSuh1PfTPmKNPeeZKq1Q8huMnFb41ws9NfsmZisyfgxz6ma39Xym9PPupzPI4y+lOudju4zlCQp757XS33EA+aYnNB+ROcOJ/Y++DyCZVWjRv06MmQz9QRwup2SVB3tI1L1/HykKQd8xbP8s0oRSWe5uq1rioUY0IxGRol6zk8pI7zWP5jQ9b07Nlzw4YNAQEBiO1wyI+SwWAQiTjxfXlRWQgvKgvhkKh6vV4sFiMOwFsqC+GKqARhXPb6bFsH6w1cEZU7Zoq4Iyp3KlTEWyor4UVlIbyoLIQXlYXwDSUWwlsqC+FFZSG8qCyEF5WF8KKyEF5UFsKLykJ4UVkIP/jAQnhLZSFc+Z5CoZALK37NcGg5S2FhIeIGnCmRRCIogRE34EVlIbyoLIQXlYXworIQXlQWwovKQnhRWQgvKgvhRWUhvKgshBeVhfCishBeVBbCi8pCeFFZCMs9no0fP/7YsWNCofGnii3fFA7Onj2L2AvLHVtMmDAhLCzM/Cv2gkeEhIQgVsNyUaOjozt37mxdGsFxp06dEKthvwualJSUoKDHfnEDAwMHDx6MWA37RQ0NDU1KSjIbK/x94YUXIiIiEKvhhLOod955x1yP+vr6Dhs2DLEdJ+3SqJTaBwV6QkdSvnbVHGxjxn+UadDjZLJXX/rH/v8daNK4qVgXduN8Ze18LPk9/E3K6i6ja3iQhk9IgHn5i738n8JhOeM4UZfmxrnyUweKyx4QuP6Rm3L0FF7Zn80deM3MbDjMriGqtd94oQjz8BFGxynaveKLnAOnEPXozvsX0stJHAklyNXL1SvYzd3PFdUH1BW60rtKdYnWUGWAJxkaI+vzvuP7Sw4WVVWm3fzFbVyPPIPloS38UX2mOK/swU0lYSDaJHu17+GDHIcjRT2wpfBqhsotwKVBbBBiC8V3yu5dLfPwFaVMb4AchMNETd/94Pyx8qZJDREbuZ5+Wygkh89xTN/JMaLuWZd/M0vdIpnN/cWsP25KpdiIuVGIcRwg6t5NBTcvqtlqo9bcOHEHQwTz9sr04EPBTc2NzEouKApEdQjVVhG/bbyLmIVpUXetuesd7o44Q9PEhjnnNTq1DjEIo6L+vjEfCvygxo5s7jOPzEPy/WJGjZVRUXOz1F7hHohjRLULUSvxkvvMGStzoh7/rYgkUECkF3JKVJWlUz9pf+7C/xANiF2E+4ylFEMwJ2r2XyqJ3BmHvxnAK9St9AFzK6SYE7VCafAMkSNO4hfhReAo/4YaMQJDU2/lZVXQH/YN80T0UF5R/Mvvy3Nvn9fpqhrHdEhOGO7vZxylK7h3Y+nKIR+8v/7Q0U0XLx/xcPePa9mtZ7ex5qVoZ8/v33vwG42mvFmTlxJeTEF0IhCiyxnK4CgmJioYstScc2pE2yAHjuNr1o+5kXumf+/pU8ZtUci9/712eFHxHYgSCY1ezrbvWtDqhVcWfpY25M05R9K/z7xkrDgL7mVvSf00vlXP6RN3xMf12rVnKaIToUhQXMhQCcyQqMpiPfb8Zj1rcDPv3P2i3MFvzmnSqKO7m0/vHh/IXT2P/bnNkiC2eVJsi64ikTgqorWPV8idu1cg8PjJHZ4egd0SR7i6ukdHtmkf3xfRCiasqiAQIzAkqkFvXKSJ6CH3VqZQKI6JjDd/hAuBeDm5j1f2hgY3tRy7uLhpqirgoKjkdmBApCU8LKQZohOYS0cChkZkGapTxQKEE3R9JU2VCsf10CGxDlTIH/edMIzi3VWry319wiwfJRIZohPcQJBMtf0ZEtU9SIjR9pq6KXxAkuEp1SrFJ/4AI5S6en2V5aNWW4nohCQIVwVDbkwZErVpO7e0n0oRPYQENdLpNJ6eAb7eoeaQ4pK71pZKiZdnUNaVYwRBmOXPupqG6ITASb8QhkyVoTpVKpVAm/7+TVp0jYlq2ySm4/af55eWFaoqy9JPpv5rzT9Onfml7rNimyfDKNLPe5bC5GN2zl/HT6YiOgFRm3dkaCaDuSWiCk+RsrDSP4KWYcLhQ7/6M+On736cdev2BT/fBq1je7zU8a26T2kc0/61V8b/eeqnDz/tAM3glAFzVn37PqKn43XveolIhPxC6a22LTA3SX76YPHJ30ubd2X56nhKrqXlefmLBkwIQ4zA3DBhfFcf6NQUXi9G3EOnwV8bwZwLaUZX6Ddqrbh2tiIwxuZ86qz5XSnDCQKHbomtni4MCSnkz20Act3myTfzMimjXGXuak05ZdTnMw8iG1w/ecfDRyhTMDeZwfQapbUzbsh8ZGHNqV/bktK/Mz/l7RWMnh/l5UUGnHruU6vVSKUyu+5Bq9Fmp+WP/SoaMQjTolapdN9+kteiO1dq1qxDuU3bKl4eyKj7fqbXKLkoJC07u108cBNxgGtpt9y8BAwrihy17vfYzgfn05TNWb3u99Khm76B0remMNTitcZhK/RP7i/J2FfSqFOIxJWFyyGuHMlVeAqHTm+IHIEj99Kc3Fucsa9U7iONaPM8WzqOpeBaUXFeRWC49M0JDrBRM47fyrhhdo66nHD1cYloXb+3SRVcLSrLV8FB10G+jVo7ctGkU+xPzUwrPbmnRFdFCsQCqVwMtusd7CaROXuxrKvSqYqrKoo0GqUW1+EwYxod65o8xPGvphPtJC8v1fyxvTj/htags/KQ8xQnwqQeiT23ZDZ3k1dL8ziJWIp5BYjjEjwca53WOK/Hs8pyXWWF1eOtvkPf8gkzOWAw+32o45sYszG5h6iVU7UcTbmR1bSnSi0UILELcvN00rKE5W7suAlXHE5yCl5UFsKLykJ4UVkILyoL4UVlIf8HAAD//62kJzkAAAAGSURBVAMAdPGmBW4OqqIAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"task_solver_llm\", task_solver_llm)\n",
    "\n",
    "builder.add_edge(START, \"task_solver_llm\")\n",
    "builder.add_edge(\"task_solver_llm\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b42c51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 32838,\n",
       " 'topic': 'Алгебра и арифметика',\n",
       " 'subtopic': 'Задачи на движение',\n",
       " 'complexity_level': '2',\n",
       " 'problem': 'Вадим и Лёша спускались с горы. Вадим шёл пешком, а Лёша съезжал на лыжах в семь раз быстрее Вадима. На полпути Лёша упал, сломал лыжи и ногу и пошёл в два раза медленней Вадима. Кто первым спустится с горы?',\n",
       " 'solution': 'Леша шёл пешком с половины горы столько же времени, сколько Вадим спускался с вершины.',\n",
       " 'answer': 'Вадим'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_task = list_json[47]\n",
    "test_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "32c498df",
   "metadata": {},
   "outputs": [],
   "source": [
    "##===========Инициализируем промты===========##\n",
    "system_msg = SystemMessage(\n",
    "    \"\"\"\n",
    "# Роль: ассистент, хорошо понимающий и разбирающийся в математике.\n",
    "\n",
    "## Твоя задача помочь пользователю \"Студент\" решить задачу по математике.\n",
    "\n",
    "## Твое решение должно содержать подробное рассуждение и шаги, которые ты предпринял, чтобы решить задачу. \n",
    "\n",
    "## Ты должен обходиться без длительных рассуждений. Вначале напиши план того, как ты будешь решать задачу. Далее действуй согласно описанному плану. Не уходи в бесконечные рассуждения.\n",
    "\n",
    "## Ответ к задаче необходимо давать после описания решения к задаче в следующем формате: ответ должен находиться между \"START_ANSWER\" и \"END_ANSWER\". Т.е. это должно выглядеть следующим образом:\n",
    "\n",
    "START_ANSWER \"Ответ ассистента к задаче\" (идет после решения) END_ANSWER\n",
    "\n",
    "## Важно!!! Твой ответ должен быть коротким и четким и отвечать на вопрос исходной задачи.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "human_msg = HumanMessage(\n",
    "    f\"Привет, уважаемый ассистент! Я студент. Помоги мне, пожалуйста, решить следующую задачу: {test_task['problem']}\",\n",
    "    name=\"Студент\",\n",
    ")\n",
    "\n",
    "# Use with chat models\n",
    "messages = [system_msg, human_msg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9ba6a861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "\n",
      "# Роль: ассистент, хорошо понимающий и разбирающийся в математике.\n",
      "\n",
      "## Твоя задача помочь пользователю \"Студент\" решить задачу по математике.\n",
      "\n",
      "## Твое решение должно содержать подробное рассуждение и шаги, которые ты предпринял, чтобы решить задачу. \n",
      "\n",
      "## Ты должен обходиться без длительных рассуждений. Вначале напиши план того, как ты будешь решать задачу. Далее действуй согласно описанному плану. Не уходи в бесконечные рассуждения.\n",
      "\n",
      "## Ответ к задаче необходимо давать после описания решения к задаче в следующем формате: ответ должен находиться между \"START_ANSWER\" и \"END_ANSWER\". Т.е. это должно выглядеть следующим образом:\n",
      "\n",
      "START_ANSWER \"Ответ ассистента к задаче\" (идет после решения) END_ANSWER\n",
      "\n",
      "## Важно!!! Твой ответ должен быть коротким и четким и отвечать на вопрос исходной задачи.\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Студент\n",
      "\n",
      "Привет, уважаемый ассистент! Я студент. Помоги мне, пожалуйста, решить следующую задачу: Вадим и Лёша спускались с горы. Вадим шёл пешком, а Лёша съезжал на лыжах в семь раз быстрее Вадима. На полпути Лёша упал, сломал лыжи и ногу и пошёл в два раза медленней Вадима. Кто первым спустится с горы?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<|startoftext|><|im_start|>system\n",
      "\n",
      "# Роль: ассистент, хорошо понимающий и разбирающийся в математике.\n",
      "\n",
      "## Твоя задача помочь пользователю \"Студент\" решить задачу по математике.\n",
      "\n",
      "## Твое решение должно содержать подробное рассуждение и шаги, которые ты предпринял, чтобы решить задачу. \n",
      "\n",
      "## Ты должен обходиться без длительных рассуждений. Вначале напиши план того, как ты будешь решать задачу. Далее действуй согласно описанному плану. Не уходи в бесконечные рассуждения.\n",
      "\n",
      "## Ответ к задаче необходимо давать после описания решения к задаче в следующем формате: ответ должен находиться между \"START_ANSWER\" и \"END_ANSWER\". Т.е. это должно выглядеть следующим образом:\n",
      "\n",
      "START_ANSWER \"Ответ ассистента к задаче\" (идет после решения) END_ANSWER\n",
      "\n",
      "## Важно!!! Твой ответ должен быть коротким и четким и отвечать на вопрос исходной задачи.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Привет, уважаемый ассистент! Я студент. Помоги мне, пожалуйста, решить следующую задачу: Вадим и Лёша спускались с горы. Вадим шёл пешком, а Лёша съезжал на лыжах в семь раз быстрее Вадима. На полпути Лёша упал, сломал лыжи и ногу и пошёл в два раза медленней Вадима. Кто первым спустится с горы?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "START_ANSWER\n",
      "Давай разберём ситуацию. Лёша спускается в 7 раз быстрее Вадима. Но он упал и спустился дважды за Вадием. Условие упадка и перестановки времени делают его время проживания более длинным. Вадим просто шёл пешком. По сути, хотя Лёша быстрее, упадок и переход по мединому пути приведут к тому, что Вадим скорее спустится.\n",
      "\n",
      "START_ANSWER Вадим.\n",
      "END_ANSWER\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "messages = graph.invoke({\"messages\": messages})\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "04d29baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Вадим.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = messages[\"messages\"][-1].content\n",
    "\n",
    "start_marker = \"START_ANSWER\"\n",
    "end_marker = \"END_ANSWER\"\n",
    "\n",
    "start_idx = answer.rfind(start_marker) + len(start_marker)\n",
    "end_idx = answer.rfind(end_marker)\n",
    "assist_answer = answer[start_idx:end_idx].strip()\n",
    "assist_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff4ae30",
   "metadata": {},
   "source": [
    "# Строим task_reviewer_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8308ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb7cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node\n",
    "def task_solver_llm(state: MessagesState):\n",
    "    return {\"messages\": [tiny_lama_model.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f4d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"task_solver_llm\", task_solver_llm)\n",
    "\n",
    "builder.add_edge(START, \"task_solver_llm\")\n",
    "builder.add_edge(\"task_solver_llm\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10991fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_task = list_json[47]\n",
    "test_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a71c0cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##===========Инициализируем промты===========##\n",
    "system_msg_describe = SystemMessage(\n",
    "    f\"\"\"\n",
    "# Роль: ассистент, проверяющий ответ другого ассистента.\n",
    "\n",
    "## Твоя задача оценить правильность ответа АССИСТЕНТА №1 и сказать, правильно ли ассистент решил задачу или нет.\n",
    "\n",
    "## Тебе на вход будет подаваться ответ АССИСТЕНТА №1, а также ground_truth ответ. Однако ground_truth ответ может иногда отсутствовать.\n",
    "\n",
    "## В случае, если ground_truth ответ имеется, то тебе нужно сравнить ответ АССИСТЕНТА №1 и ground_truth ответ. \n",
    "\n",
    "## В случае, если В случае, если ground_truth ответ отсутствует, то тебе нужно сравнить ответ АССИСТЕНТА №1 и ground_truth ответ.\n",
    "\n",
    "## В качестве ответа напиши \"START_ANSWER ПРАВИЛЬНО END_ANSWER\", если ответ АССИСТЕНТА №1 и ground_truth совпадают. \n",
    "\n",
    "## В качестве ответа напиши \"START_ANSWER ОШИБКА END_ANSWER\", если ответ АССИСТЕНТА №1 и ground_truth НЕ совпадают.\n",
    "\n",
    "## ПИШИ В ОТВЕТЕ ТОЛЬКО \"START_ANSWER ПРАВИЛЬНО END_ANSWER\" и \"START_ANSWER ОШИБКА END_ANSWER\".\n",
    "\n",
    "## Ты должен обходиться без длительных рассуждений. Твоя задача просто сравнить два ответа и сказать правильно ли АССИСТЕНТ №1 решил задачу или нет.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "system_msg_w_answer = HumanMessage(\n",
    "    f\"Ответ АССИСТЕНТА №1: {assist_answer}. ground_truth = {test_task['answer']}\",\n",
    "    name=\"АССИСТЕНТ №1\",\n",
    ")\n",
    "\n",
    "# Use with chat models\n",
    "messages = [system_msg_describe, system_msg_w_answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4ea6d782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "\n",
      "# Роль: ассистент, проверяющий ответ другого ассистента.\n",
      "\n",
      "## Твоя задача оценить правильность ответа АССИСТЕНТА №1 и сказать, правильно ли ассистент решил задачу или нет.\n",
      "\n",
      "## Тебе на вход будет подаваться ответ АССИСТЕНТА №1, а также ground_truth ответ. Однако ground_truth ответ может иногда отсутствовать.\n",
      "\n",
      "## В случае, если ground_truth ответ имеется, то тебе нужно сравнить ответ АССИСТЕНТА №1 и ground_truth ответ. \n",
      "\n",
      "## В случае, если В случае, если ground_truth ответ отсутствует, то тебе нужно сравнить ответ АССИСТЕНТА №1 и ground_truth ответ.\n",
      "\n",
      "## В качестве ответа напиши \"START_ANSWER ПРАВИЛЬНО END_ANSWER\", если ответ АССИСТЕНТА №1 и ground_truth совпадают. \n",
      "\n",
      "## В качестве ответа напиши \"START_ANSWER ОШИБКА END_ANSWER\", если ответ АССИСТЕНТА №1 и ground_truth НЕ совпадают.\n",
      "\n",
      "## ПИШИ В ОТВЕТЕ ТОЛЬКО \"START_ANSWER ПРАВИЛЬНО END_ANSWER\" и \"START_ANSWER ОШИБКА END_ANSWER\".\n",
      "\n",
      "## Ты должен обходиться без длительных рассуждений. Твоя задача просто сравнить два ответа и сказать правильно ли АССИСТЕНТ №1 решил задачу или нет.\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: АССИСТЕНТ №1\n",
      "\n",
      "Ответ АССИСТЕНТА №1: Вадим.. ground_truth = Вадим\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<|startoftext|><|im_start|>system\n",
      "\n",
      "# Роль: ассистент, проверяющий ответ другого ассистента.\n",
      "\n",
      "## Твоя задача оценить правильность ответа АССИСТЕНТА №1 и сказать, правильно ли ассистент решил задачу или нет.\n",
      "\n",
      "## Тебе на вход будет подаваться ответ АССИСТЕНТА №1, а также ground_truth ответ. Однако ground_truth ответ может иногда отсутствовать.\n",
      "\n",
      "## В случае, если ground_truth ответ имеется, то тебе нужно сравнить ответ АССИСТЕНТА №1 и ground_truth ответ. \n",
      "\n",
      "## В случае, если В случае, если ground_truth ответ отсутствует, то тебе нужно сравнить ответ АССИСТЕНТА №1 и ground_truth ответ.\n",
      "\n",
      "## В качестве ответа напиши \"START_ANSWER ПРАВИЛЬНО END_ANSWER\", если ответ АССИСТЕНТА №1 и ground_truth совпадают. \n",
      "\n",
      "## В качестве ответа напиши \"START_ANSWER ОШИБКА END_ANSWER\", если ответ АССИСТЕНТА №1 и ground_truth НЕ совпадают.\n",
      "\n",
      "## ПИШИ В ОТВЕТЕ ТОЛЬКО \"START_ANSWER ПРАВИЛЬНО END_ANSWER\" и \"START_ANSWER ОШИБКА END_ANSWER\".\n",
      "\n",
      "## Ты должен обходиться без длительных рассуждений. Твоя задача просто сравнить два ответа и сказать правильно ли АССИСТЕНТ №1 решил задачу или нет.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Ответ АССИСТЕНТА №1: Вадим.. ground_truth = Вадим<|im_end|>\n",
      "<|im_start|>assistant\n",
      "START_ANSWER ПРАВИЛЬНО  \n",
      "(Поскольку ground_truth и ответ АССИСТЕНТА совпадают по содержанию, хотя форматирование немного различается)  \n",
      "END_ANSWER\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "messages = graph.invoke({\"messages\": messages})\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e681ab",
   "metadata": {},
   "source": [
    "# Объединяем task_solver_llm и task_reviewer_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1828f504",
   "metadata": {},
   "source": [
    "--------------\n",
    "--------------\n",
    "--------------\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b7489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Строим агентный граф\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "# Node\n",
    "def task_solver_llm(state: MessagesState):\n",
    "    pass\n",
    "\n",
    "\n",
    "def task_reviewer_llm(state: MessagesState):\n",
    "    pass\n",
    "\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"task_solver_llm\", task_solver_llm)\n",
    "builder.add_node(\"task_reviewer_llm\", task_reviewer_llm)\n",
    "\n",
    "builder.add_edge(START, \"task_solver_llm\")\n",
    "builder.add_edge(\"task_solver_llm\", \"task_reviewer_llm\")\n",
    "builder.add_edge(\"task_reviewer_llm\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74a76ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4315d8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2cdcd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 32838,\n",
       " 'topic': 'Алгебра и арифметика',\n",
       " 'subtopic': 'Задачи на движение',\n",
       " 'complexity_level': '2',\n",
       " 'problem': 'Вадим и Лёша спускались с горы. Вадим шёл пешком, а Лёша съезжал на лыжах в семь раз быстрее Вадима. На полпути Лёша упал, сломал лыжи и ногу и пошёл в два раза медленней Вадима. Кто первым спустится с горы?',\n",
       " 'solution': 'Леша шёл пешком с половины горы столько же времени, сколько Вадим спускался с вершины.',\n",
       " 'answer': 'Вадим'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94e5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-mas-hse-project-Nc-XwFR1-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
